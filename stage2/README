________________________________________________________________

		    Team - Red Project Stage II  
		Word sense disambiguation based on Word2Vec
________________________________________________________________

Authors :  Ankit Anand Gupta
		Brandon Paulsen
		Sai Ram Kowshik Vattipally
		Sandeep Vuppula		

*************
CONTRIBUTIONS
*************
Ankit Anand Gupta : 
Brandon Paulsen : 
Sai Ram Kowshik Vattipally : 
Sandeep Vuppula : 
********
CONTENTS
********

Contents in the Directory:
* install.sh
* runit.sh
* runall.sh
* README
* RESULTS
* postProcessing.py
* def_gen.py
* stopwords.txt (The list of stop words we are excluding)
* senseclusters_scorer
* src
	DefinitionGeneration.py
	cluster.py
	file_processing.py
	sensesample.py
	sentenceIter.py
	tokenizing.py
* output
* input
	abandon-verb-pauls658.xml
	racket-noun-pauls658.xml
	sidewalk-furnace-pauls658.xml
	
	wear-verb-vuppu008.xml
	plot-noun-vuppu008.xml
	television-food-vuppu008.xml

	strike-verb-gupta299.xml
	bat-noun-gupta299.xml
	banana-wall-gupta299.xml

	shoot-verb-vatti001.xml
	date-noun-vatti001.xml
	pigeon-car-vatti001-1.xml

	line-noun-6sense.xml
	pmss-noun-4senses.txt

********************
PROBLEM AND SOLUTION
********************
The problem we are trying to solve is to create a definitions for the different senses of a target word, given several instances of the target word. This also includes discovering how many senses of the target word are present in the given instances. The system takes a XML file containing a list of contexts each of which contain the target word within a <head></head> tag. Each instance is a sentence or a group of sentences that surround the target word which we intend to find the meaning of in the given context. The system we design is intended to group these contexts based on the sense of the target word in that particular context. Each of the resultant clusters will contain contexts where the target word is used in same sense. This step is called clustering. Our system then uses the clusters formed to create a unique definition for each unique sense of the target word.

Our approach for clustering is based on using Word2Vec. Word2Vec models generate word embeddings which can be used to find similarities between various words. We use a Word2Vec model that has been generated by google as our additional data in addition to the data we generate from our corpus to better understand the senses of words.

To perform clustering, we use an algorithm called Affinity Propogation. Affinity propogation works based on the concept of message passing between data points. Affinity propogation also discovers the number of clusters on its own, effectively generating the number of senses a word may have. After this we get a fixed number of clusters along with the contexts that belong to those clusters.

We use the addition feature of Word2Vec to pick upto 15 frequently occuring words that are nouns, adjectives and verbs (5 each) from contexts of every sense cluster generated for each word. Similar to Stage 1 ,Context Free Grammer(CFG)  a We then use these words in a pre-defined to form a definition for each sense of our words.



-----------------------------------
Point-by-point outline of execution
-----------------------------------


Definition Generation:
In src/DefinitionGeneration.py file:

Section I : get_Noun_Verb()
			Used for Stage I
			The function is used to seperate the Nouns, Verbs and Adjectives in the given set of topic words.
			We use the Parts of Speech Tagger from the Natural Language Toolkit to tag the POS for each word in the set of topic words.
			It takes a set of topic words, assigns respective POS tags to each of those words. And then for each tag(Noun/Verb/Adjective), it forms a string of  words seperated by '|'
Section II: cfg_rule()
			The function is used to map the Context Free Grammar production rules for the english grammar to python representation.
			It takes a rule of the form "PRO  -> with | to" and splits the rules on the right hand side based on "|". Then it appends the rules present on the right side to the rule present on the left.
Section III: gen_def()
			The function is used to generate the definition of a sentence recursively using the CFG rules. It chooses a production rule randomly from the list and then recursively gen_def() function is called. In order to avoid the same word being picked twice, we remove the word from the list of Nouns/Verbs/Adjectives once it is used.			
Section IV:	process()
			The function takes the ctxes and doc_counts as the parameters. The doc_counts is a dictionary in the format (word,pos)->count. We will sort the doc_counts and then separate nouns, verbs, adjectives into their respective lists. Then we use them to create two parts of the definition. 
				Part 1: Using most_similar word from Word2Vec using topmost nouns(upto 5)
				Part 2: Sentence using the CFG grammar.
			Concatenate Part 1 and Part 2 and return the complete definition.
Section V : createPartOne()
			This function is the core part for first part of definition generation. We take the top most occurring 5 nouns and then get the most similar word which is related to those words from Word2Vec. If there are no nouns at all (possible if cluster formed has very less number of instances) then we return 'unknown'
Section IV: generate_Definition()
			This is the main class which drives the definition generation. First it takes a list of topic words and removes the target word from the list. Later it retrieves the Noun/Verb/Adjective words in the form of concatenated string separated by "|".  Then it takes the production rules and maps them as Array item -> set of tuples. Then gen_def() function proceeds to generate the definition based on the set of topic words.


--------
Word2Vec
--------
Word2Vec is a set of models that are used to produce word embeddings. Word embeddings is a representation of a raw text in some mathematical format, usually vectors. Word2vec models take a corpus of text as input and produces a vector spaces containing one vector for each word. The words that are close in context to each other are placed close to each other in the vector space as well. Hence similarity between two words can be measured by measuring the distance between them in the vector space. 

Word2Vec was created by a Tomas Mikolov et al. at Google. We use the Gensim implementaion of Word2vec for our system.


From these topic words we generate sentences which are the definitions of the target word in each cluster. The core of our definition generation system is a context free grammar created by ankit which defines a simple sentence structure. We then take the , POS tag them, and then generate random sentences using our context free grammar. 
The input file may be of 2 kinds. The first one has only one target word which is either a noun or verb which has multiple senses. The second kind is where there are contexts that uses two different words that are conflated into a single word. In the first kind, we try to cluster the contexts based on various senses the target word we have. In the second one, we try to assign a sense to the conflated word and try to group the contexts into clusters such that each cluster might have same sense as each word that the conflated word is formed from. Note that we might generate more or less clusters than the number of words used for conflating the word.



---------------------------------
Clustering - Affinity Propogation
---------------------------------
Affinity propogation is a clustering algorithm based on the property of message passing between data points. It doesn't require a pre-defined number of clusters for clustering unlike algorithms like k-means.

Assume data points d_1 through d_n. Affinity propogation uses a similarity function S such that 
	
			S(d_i,d_j) > S(d_i,d_k) 
	is true only when d_i is more similar to d_j than d_k.

Affinity propagation finds exemplars from data points and forms clusters by gathering data points around these exemplars. An exemplar is any data point which is a representative of cluster. Initially all the data points are considered as potential clusters. One data point is formed from one context. These data points are then matched to a closest exemplar based on some kind of similarity function using the distance between the points to perform clustering. 

The algorithm initializes and updates two matrices as it works. Both the matrices are initialized to all zeroes initially.
1) A responsibility matrix R ,and
2) An availability matrix A

R has values r(x,y) which reresents how well suited the data point d_x is for d_y comared to other exemplars.

A has values a(x,y) which represent how well suited is d_x as an exemplar for d_y considering the preference of other data points preference for d_y as an exemplar.

Preference value: The preference value of data point d_i is the suitability of that point to become an exemplar (origin of a cluster). Hence the higher the preference value the higher the probability that it becomes an exemplar. We calculate our preference value as the negative of square of the Euclidean distance between current word vector and the vector that is most dissimilar to given word multiplied by 0.25.  

			Preference Value = -(Dist)^2 

We then multiply our preference value with 0.25 to determine a increment value for the current word.

		Increment Value = Preference value * 0.25

The value 0.25 was experimentally determined to be the best performing value. Then, we experiment with 10 preferences values(5 on each side of the original value) that result from adding or subtracting the increment value upto 5 times.

We use each of these values as our preference value in each iteration and examine the clusters generated by calculating the Calinski Harabaz score for the generated clusters. This score is defined as the ratio between dispersion within a single cluster and the dispersion between multiple clusters. The preference value with the highest Calinski Harabaz score is selected as the preference value for that particular word.



To perform clustering, we use a sci-kit learn's implementation of Affinity Propogation.

---------------------
Definition generation
---------------------

Our previous approach to definition generation was to select the topic word that hdp-wsi gave us. The we used CFG rules to make a sentence that was our definition for it. Since in the stage-2 of our project, we use Word2Vec model for our clustering and not hdp-wsi. We decided  to use the vector addition feature of Word2Vec. Our intention was to pick upto 5 highest occuring Nouns, Adjectives and Verbs from context generated for every sense cluster generated for each word, and then generate the definition.

Our definition has two parts now, The first part is a mostly hardcoded sentence that tries to use the hypernym(ideally) of the highest occuring 5 nouns(or less if there are not enough nouns). We did this to get the overall meaning of the word and use it in the definition. For example the definition of 'dog' given as an example had the word 'canine' in it which is actually a hypernym for 'dog' just like the word 'hound' . Then the second part is similar to our approach in the stage-1 of our project. We use some of the highest occuring words(Nouns, Adjectives and Verbs) and make a sentence from it using CFG rules. 

Vector addition and subtraction of words could be used to get different Hypernyms or Hyponyms of those words. We wanted to add the top nouns that we got to get their hypernym, so that it could be used in the first half of the definition, the reason for this is explained in the previous paragraph.

Initially we were using all the words from the clusters towards the total frequency counts. This gave us slightly poor results as some instances had multiple occurences of an irrelevant word, and that word was being selected as our top word for the vector summation. An example of this is the word 'colbert' in one cluster of the word 'wear'. It was occuring 4 times in one instance and hence was one of the most occurring noun, but was totally irrelevant. So we got an idea from the sentiment analysis video, where it was mentioned that he frequency of a word within an instance was irrelevant, the useful piece of information was if that word existed or not. So we decided that the frequency count for each word would be increased by max 1 for each instance. This approach gave us better definitions.

In our clustering results We got varying cluster sizes. In one case the cluster was only two instances so the context words for that cluster didn't have any nouns whatsoever. So it was not possible to generate the word for the first part of the definition through the vector sum process. So the word 'unknown' was used instead, this approach is justified as the cluster with only 2 instances is definitely an erroneous one and therefore that word sense mostly doesn't exist and hence should not have any definition.        

The basic idea for second part of definition generation of a word is to use the context words of the sentence in which the word is used. We use the Context Free Grammars(CFG) to generate the definition of the words. The CFGs have a set of recursively defined rules(productions) to generate string patterns.

The Context Free Grammar Rules used are:

S 	-> S1
S1 	-> NP VP
NP 	-> Det ADJ N
VP 	-> V NP
Det -> a | the | is
N 	-> Noun words list
V 	-> Verb words list
ADJ -> Adjective words list


We use the Context Free Grammar rules to preserve the syntactic structure of the English grammar. The English grammar has rules such as, Noun phrase should be followed by a Verb Phrase, 'a', 'an', 'the' are Determiners and so on. To preserve such rules and to use them during the sentence generation, we use the Context Free Grammars. The CFG rules are recursively applied to the topic words to generate the definition of the topic words. 

Firstly, we derive the Parts of Speech for each of the topic words and then seperate the words into different sets of Noun/Verb/Adjective based on the dervied Parts of Speech tags.

We iterate over each of the symbols(Terminal symbol) and choose a Non-Terminal or Terminal symbol based on the production rule to generate the definition. 

Examples of Terminal Symbols: S, S1,, NP, VP, Det, N, V, ADJ
Non-Terminal Symbols : Nouns, verbs, adjectives, determiners

Example:

Noun Words	: [('gun', 10), 
			   ('rifle', 6), 
			   ('times', 5), 
			   ('deer', 4), 
			   ('guy', 4)]

Definition  : the young times kill the dead deer 


**********
HOW TO RUN
**********

1) In terminal, navigate to the directory of project.
2) Run install.sh as "./install.sh". This simply installs all the necessary packages for running
3) To run word sense induction on a single senseval2 xml file, simply execute:
	./runit.sh <senseval2-xml-file>

Eg: To run our project on the file "date-noun-vatti001.xml", run as 
	./runit.sh input/date-noun-vatti001.xml

- You can also run the tool on every file in the input directory with the following command:
	./runall.sh

4) The reults of cluster creation, assigment of contexts to clusters, and the definitions generated for each cluster will be written in files in the output directory. See the section on output file format for an explanation of the format


********
EXAMPLES
********
-----------------
INPUT FILE FORMAT
-----------------

The input files are taken from the directory: ./input/

The input can be a file either of the following two kinds:

1)An XML with a single Noun or Verb tagged: (single word used in multiple senses)

Example file format:
--------------------

<corpus lang="english">
<lexelt item="line">
<instance id="1">
<answer instance="1" senseid="photograph or videograph a scene or a movie"/>
<context>
, " he says of his subjects. " When they look back, it's more than a portrait of them, it's a moment in place and time. " # As for Leon Borenzstein, he tells people who simply want to look good to go someplace else. But that doesn't stop him from getting clients. # " People are tired of sterile portraits, " he says. " They want something more creative. " # Whether they know it or not, apparently. When San Francisco fashion executive Naomi Mann hired Margretta Mitchell to <head>shoot</head> a family portrait, she asked for a couple of relatively straightforward photographs posed in the living room and garden. But Mitchell happened to snap the five Mann children grouped around a staircase while wearing big grins and goofy hats, and Mann liked it so much she bought that one instead. # " Even if you don't know our family, it speaks to you about who we are, " she says. " It's playful. It's funny. When I saw it, I said,
</context>
</instance>
<instance id="2">
<answer instance="2" senseid="photograph or videograph a scene or a movie"/>
<context>
o, Lesley Gore, fabulous woman, passed away over the weekend. And we also lost Louis Jourdan. Now do you remember a film called " Gigi? " Yes, and also was it " Octopussy, " was he also in that? Yes, he's, he was an amazing Frenchman. We also lost him this weekend. But the good news here is that Naya and Ryan, you both have a lot going on right now. Naya first, what's going on? Well, I am after this going to go and <head>shoot</head> a couple episodes of Lifetime's " Devious Maids. " So I'm excited about that, and it'll be cool to do something different, and all the ladies seem awesome. So I'm really looking forward to it. Yeah, they're all good girls. And what about you, Ryan? You can catch me on " General Hospital " as detective Nathan West. It's February, it's GH Fan February, so if you're a fan of the show, anything you
</context>
</instance>


2) An XML with a Name-conflate pair: (two words conflated into one used in two different senses)


Example file format:
--------------------

<?xml version="1.0" encoding="iso-8859-1" ?>
<corpus lang='english'>

<lexelt item="p_c">

<instance id="1">
<answer instance="1" senseid="pigeon"/>
<context>
 the genome and tissues, as well as the potential parenting, of the band-tailed <head>p_c</head> Patagioenas fasciata. # I've joined forces with a sweep of other interested scientists    fixed. But it is getting prettier. Now traveling under the name of 
</context>
</instance>

<instance id="2">
<answer instance="2" senseid="pigeon"/>
<context>
to beat either in the field or for dollar value -- and splice them into the genome of a stem cell from a common rock <head>p_c</head> # Rock pigeon stem cells containing this doctored genome could be transformed into germ    , and I know they are coming in on my frequency 
</context>
</instance>


******************
OUTPUT FILE FORMAT
******************

The output files are saved in the following directory: ./output/

runit.sh will generate two output files for a given input file: one that shows the assignments of contexts into clusters, and one that shows the definition for each cluster. The assignment file should be named <target-word>-<pos>-assignments.xml. This is a senseval2 formatted file which is basically a list of instances where each instance has a context, and a cluster ID. The output file is identical to the input file, except that the senseid attributes for each instance have been replaced with a number which denotes membership to a specific cluster. The number is the ID of the cluster that the context belongs to. In addition, the instance's retain the same ID as the had in the original input file. In the following example, the instance with id=2 in the input file has been assigned to cluster 1.

<corpus lang="english">
<lexelt item="LEXELT">
<instance id="2">
<answer instance="2" senseid="1"/>
<context>
I used to draw a comparison between him and Hindley Earnshaw, and perplex myself to explain satisfactorily why their conduct was so opposite in similar circumstances.  They had both been fond husbands, and were both attached to their children; and I could not see how they shouldn't both have taken the same road, for good or evil.  But, I thought in my mind, Hindley, with apparently the stronger head, has shown himself sadly the worse and the weaker man.  When his ship struck, the captain <head>abandoned</head> his post; and the crew, instead of trying to save her, rushed into riot and confusion, leaving no hope for their luckless vessel.  Linton, on the contrary, displayed the true courage of a loyal and faithful soul: he trusted God; and God comforted him.  One hoped, and the other despaired: they chose their own lots, and were righteously doomed to endure them. But you'll not want to hear my moralising, Mr. Lockwood; you'll judge, as well as I can, all these things: at least, you'll think you will, and that's the same.  The end of Earnshaw was what might have been expected; it followed fast on his sister's: there were scarcely six months between them.  We, at the Grange, never got a very succinct account of his state preceding it; all that I did learn was on occasion of going to aid in the preparations for the funeral.  Mr. Kenneth came to announce the event to my master.
</context>
</instance>

The defintions file should be named <target-word>-<pos>.defs. This file contains the definition generated for each cluster. In the following example, cluster 1 has the definition "a land england with a hands and a death want with is john", and so on. The number next to the word represents the cluster ID, so going back to the previous example, the definition of the sense of abandon being used in instance 2 corresponds to the previously mentioned definition. 


*********
CITATIONS
*********
1) The main idea for Word2Vec is based on the following paper by Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado and Jeffrey Dean: Distributed representations of words and phrases and their compositionality (https://arxiv.org/pdf/1310.4546.pdf)

@inproceedings{mikolov2013distributed,
  title={Distributed representations of words and phrases and their compositionality},
  author={Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
  booktitle={Advances in neural information processing systems},
  pages={3111--3119},
  year={2013}
}


2)Our main idea for clustering uses Affinity Propogation:(http://www.icmla-conference.org/icmla07/FreyDueckScience07.pdf)

@article{frey2007clustering,
  title={Clustering by passing messages between data points},
  author={Frey, Brendan J and Dueck, Delbert},
  journal={science},
  volume={315},
  number={5814},
  pages={972--976},
  year={2007},
  publisher={American Association for the Advancement of Science}
}





