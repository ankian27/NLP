________________________________________________________________

		    Team - Red Project Stage II  
		Word sense disambiguation based on Word2Vec
________________________________________________________________

Authors :  	Ankit Anand Gupta
		Brandon Paulsen
		Sai Ram Kowshik Vattipally
		Sandeep Vuppula		

*************
CONTRIBUTIONS
*************
Ankit Anand Gupta :  Wrote the writeup for discussion of Definition generation results in RESULTS-2. Co-authored the file DefinitionGeneration.py along with Sandeep. Came up with an idea to find the subject of a verb and use it for definition generation but we couldn't use the idea since the subjects we picked weren't good.
Brandon Paulsen : Wrote most of the code in cluster_baseline.py, cluster.py, file_processing.py and tokenizing.py. Experimented with various word2vec  models that we used for testing our system and finally decided to use the google model. Tested the system 
Sai Ram Kowshik Vattipally : Wrote most of this README, including how Affinity propogation and Word2Vec works. Studied various clustering algorithms before deciding to use Affinity Propogation. Author of ORIGINS, Co-authored the clustering part of RESULTS-2 along with Brandon. 
Sandeep Vuppula : Wrote the writeup discussing the best, worst and average definitions generated in RESULTS-2. Co-authored the file DefinitionGeneration.py along with Ankit. Came up with the idea to use most frequent bigrams and trigrams in definition generation but we couldn't use the idea since the n-grams we were picking weren't the represntaives of the potential definitions we were expecting. 
********
CONTENTS
********

Contents in the Directory:
* README
* RESULTS-2
* stopwords.txt (The list of stop words we are excluding)
* senseclusters_scorer
* ORIGINS
* OUTPUT
* src
	DefinitionGeneration.py
	cluster.py
	file_processing.py
	tokenizing.py
* input
	abandon-verb-pauls658.xml
	racket-noun-pauls658.xml
	sidewalk-furnace-pauls658.xml
	
	wear-verb-vuppu008.xml
	plot-noun-vuppu008.xml
	television-food-vuppu008.xml

	strike-verb-gupta299.xml
	bat-noun-gupta299.xml
	banana-wall-gupta299.xml

	shoot-verb-vatti001.xml
	date-noun-vatti001.xml
	pigeon-car-vatti001-1.xml

	line-noun-6sense.xml
	pmss-noun-4senses.txt

********************
PROBLEM AND SOLUTION
********************
The problem we are trying to solve is to create a definitions for the different senses of a target word, given several instances of the target word. This also includes discovering how many senses of the target word are present in the given instances. The system takes a XML file containing a list of contexts each of which contain the target word within a <head></head> tag. Each instance is a sentence or a group of sentences that surround the target word which we intend to find the meaning of in the given context. The system we design is intended to group these contexts based on the sense of the target word in that particular context. Each of the resultant clusters will contain contexts where the target word is used in same sense. This step is called clustering. Our system then uses the clusters formed to create a unique definition for each unique sense of the target word.

Our approach for clustering is based on using Word2Vec. Word2Vec models generate word embeddings which can be used to find similarities between various words. We use a Word2Vec model that has been generated by google as our additional data in addition to the data we generate from our corpus to better understand the senses of words. 

To perform clustering, we use an algorithm called Affinity Propogation. Affinity propogation works based on the concept of message passing between data points. Affinity propogation also discovers the number of clusters on its own, effectively generating the number of senses a word may have. After this we get a fixed number of clusters along with the contexts that belong to those clusters.

Affinity propogation clusters vectors, which means we need to represent each instance we want to cluster as a vector. This is where we utilize Word2Vec. For each instance, we tokenize it, and then choose a subset of those tokens which we believe are representative of the context. For each context, we calculate a single vector from a weighted average of all its word embeddings. We obtain the word embeddings from Word2Vec.

We use the addition feature of Word2Vec to pick upto 15 frequently occuring words that are nouns, adjectives or verbs from contexts of every sense cluster generated for each word. Similar to Stage 1 , we use these words in a pre-defined context free grammer(CFG) to form a definition for each sense of our words.

-----------------------------------
Point-by-point outline of execution
-----------------------------------
Clustering:
The main entry point is in cluster_baseline.py, which is marked ENTRY. All of the files are processed in one run of cluster_basseline.py to avoid loading the model more than once.

Starting from the ENTRY marker, we load the word2vec model. We then list all of the files in the input directory, and call cluster_tfidf() on them with the loaded model.

Section IIV: cluster_tfidf()
	This is the main cluster function. It takes a sense2val file and clusters it. It calls several sub routines to do this. First, it calls do_filename(). This function pulls all of the relevant information from the input file's name. Specifically, it pulls the target word, the part of speech, and/or the conflate words. We then load the stopwords into a set.
	
	Next, the first significant processing is done on the instances of the file. This point in the code is marked Section XYZ, and it is the section where we tokenize the instances of the file. This is done by calling the function tokenize_ctxes(). This function converts the instances in the file into lists of tokens. In addition, it POS tags each token, and removes stopwords. After this, we have a list of POS tagged contexts, but we also need raw contexts. The next for loop creates this list of raw contexts.
	
	The next significant processing is done at Section ABC. This is where we call tf_idfs() on the tokens we extracted. This function takes a list of lists of raw tokens, and returns a the same list, but each token has a tfidf weight. After calculating tfidfs, we remove any context words which aren't in the word2vec model, and we reduce the size of any contexts to have at most WORD_VEC_SIZE words.
	
	At section DEF, we convert each context to a single vector with a call to make_context_vecs_tfidf(). We now have a representation of each context which we can cluster.
	
	Before clustering with affinity propogation, we need to calculate a preference value to control the number of clusters we get. This code is marked as Section JFK. We calculate a range of prefernces values to try based on the negative square of the distance between the two most disimilar vectors. We then calculate a step size based on an increment value, and use this come up with 5 values below and 5 value above the previously calculated value. These are the preferences we try.
	
	The next step is to run affinity propagation with the different prefence values. For each preference, we calculate the Calinski Harabaz score, and save it. After running all of the preference values, we take the clusters with the highest score to be the answer we accept. In addition, we only will cluster up to a number of instances defined by PRIMING_SIZE. If there are more than this, we cluster the first PRIMING_SIZE instances, and then we handle the rest in the next step.
	
	If there are still more instances to assign to clusters, we handle this in Section LMAO. Here, we assign any unassigned instances to the clusters generated in the previous step. We represent each cluster as a single vector by taking the average of all its context vectors. Then we assign each unassigned context to the cluster which it is most similar to based on the cosine similarity between the context's vector and the cluster's vector.
	
	The last step in clustering is to call the score_clusters() function. This calls some very simple functions which write out an answer and key file for sensecluster_scorer, and then it runs the scoring script.
	
	Next, we generate the definitions in section PQR. We need to give the definition routine the word counts and document counts for every word in a cluster. These statistics are calculate for each cluster and then passed to the definition routines. The routines themselves are documented below.
	

Definition Generation:
In src/DefinitionGeneration.py file:

Section I : get_Noun_Verb()
			Used for Stage I
			The function is used to seperate the Nouns, Verbs and Adjectives in the given set of topic words.
			We use the Parts of Speech Tagger from the Natural Language Toolkit to tag the POS for each word in the set of topic words.
			It takes a set of topic words, assigns respective POS tags to each of those words. And then for each tag(Noun/Verb/Adjective), it forms a string of  words seperated by '|'
Section II: cfg_rule()
			The function is used to map the Context Free Grammar production rules for the english grammar to python representation.
			It takes a rule of the form "PRO  -> with | to" and splits the rules on the right hand side based on "|". Then it appends the rules present on the right side to the rule present on the left.
Section III: gen_def()
			The function is used to generate the definition of a sentence recursively using the CFG rules. It chooses a production rule randomly from the list and then recursively gen_def() function is called. In order to avoid the same word being picked twice, we remove the word from the list of Nouns/Verbs/Adjectives once it is used.			
Section IV:	process()
			The function takes the ctxes and doc_counts as the parameters. The doc_counts is a dictionary in the format (word,pos)->count. We will sort the doc_counts and then separate nouns, verbs, adjectives into their respective lists. Then we use them to create two parts of the definition. 
				Part 1: Using most_similar word from Word2Vec using topmost nouns(upto 5)
				Part 2: Sentence using the CFG grammar.
			Concatenate Part 1 and Part 2 and return the complete definition.
Section V : createPartOne()
			This function is the core part for first part of definition generation. We take the top most occurring 5 nouns and then get the most similar word which is related to those words from Word2Vec. If there are no nouns at all (possible if cluster formed has very less number of instances) then we return 'unknown'
Section IV: generate_Definition()
			This is the main class which drives the definition generation. First it takes a list of topic words and removes the target word from the list. Later it retrieves the Noun/Verb/Adjective words in the form of concatenated string separated by "|".  Then it takes the production rules and maps them as Array item -> set of tuples. Then gen_def() function proceeds to generate the definition based on the set of topic words.


--------
Word2Vec
--------
Word2Vec is a set of models that are used to produce word embeddings. Word embeddings is a representation of a raw text in some mathematical format, usually vectors. Word2vec models take a corpus of text as input and produces a vector spaces containing one vector for each word. The words that are close in context to each other are placed close to each other in the vector space as well. Hence similarity between two words can be measured by measuring the distance between them in the vector space. 

Word2Vec was created by a Tomas Mikolov et al. at Google. We use the Gensim implementaion of Word2vec for our system.

---------------------------------
Clustering - Affinity Propogation
---------------------------------
Affinity propogation is a clustering algorithm based on the property of message passing between data points. It doesn't require a pre-defined number of clusters for clustering unlike algorithms like k-means.

Assume data points d_1 through d_n. Affinity propogation uses a similarity function S such that 
	
			S(d_i,d_j) > S(d_i,d_k) 
	is true only when d_i is more similar to d_j than d_k.

Affinity propagation finds exemplars(origin of clusters) from data points and forms clusters by gathering data points around these exemplars. An exemplar is any data point which is a representative of cluster. Initially all the data points are considered as potential exemplars. These data points are then matched to a closest exemplar based on some kind of similarity function using the distance between the points to perform clustering. 

The algorithm initializes and updates two matrices as it works. Both the matrices are initialized to all zeroes initially.
1) A responsibility matrix R ,and
2) An availability matrix A

R has values r(x,y) which reresents how well suited the data point d_x is for d_y comared to other exemplars.

A has values a(x,y) which represent how well suited is d_x as an exemplar for d_y considering the preference of other data points preference for d_y as an exemplar.

Preference value: The preference value of data point d_i is the suitability of that point to become an exemplar (origin of a cluster). Hence the higher the preference value the higher the probability that it becomes an exemplar. We calculate our preference value as the negative of square of the Euclidean distance between current word vector and the vector that is most dissimilar to given word multiplied by 0.25.  

			Preference Value = -(Dist)^2 

We then multiply our preference value with 0.25 to determine a increment value for the current word.

		Increment Value = Preference value * 0.25

The value 0.25 was experimentally determined to be the best performing value for calculating increment. Then, we experiment with 10 preferences values(5 on each side of the original value) that result from adding or subtracting the increment value upto 5 times.

We use each of these values as our preference value in each iteration and examine the clusters generated by calculating the Calinski Harabaz score for the generated clusters. This score is defined as the ratio between dispersion within a single cluster and the dispersion between multiple clusters. The preference value with the highest Calinski Harabaz score is selected as the preference value for that particular word.

Damping factor: The higher the daming factor the less likely that a data point will shift from one exemplar to another. We use the value of 0.75 as our damping factor. This value was also experimentally determined on our 100 instances data.

To perform clustering, we use a sci-kit Learn's implementation of Affinity Propogation.

---------------------
Definition generation
---------------------

Our previous approach to definition generation was to select the topic word that hdp-wsi gave us. The we used CFG rules to make a sentence that was our definition for it. Since in the stage-2 of our project, we use Word2Vec model for our clustering and not hdp-wsi. We decided  to use the vector addition feature of Word2Vec. Our intention was to pick upto 5 highest occuring Nouns, Adjectives and Verbs from context generated for every sense cluster generated for each word, and then generate the definition.

Our definition has two parts now, The first part is a mostly hardcoded sentence that tries to use the hypernym(ideally) of the highest occuring 5 nouns(or less if there are not enough nouns). We did this to get the overall meaning of the word and use it in the definition. For example the definition of 'dog' given as an example had the word 'canine' in it which is actually a hypernym for 'dog' just like the word 'hound' . Then the second part is similar to our approach in the stage-1 of our project. We use some of the highest occuring words(Nouns, Adjectives and Verbs) and make a sentence from it using CFG rules. 

Vector addition and subtraction of words could be used to get different Hypernyms or Hyponyms of those words. We wanted to add the top nouns that we got to get their hypernym, so that it could be used in the first half of the definition, the reason for this is explained in the previous paragraph.

Initially we were using all the words from the clusters towards the total frequency counts. This gave us slightly poor results as some instances had multiple occurences of an irrelevant word, and that word was being selected as our top word for the vector summation. An example of this is the word 'colbert' in one cluster of the word 'wear'. It was occuring 4 times in one instance and hence was one of the most occurring noun, but was totally irrelevant. So we got an idea from the sentiment analysis video, where it was mentioned that he frequency of a word within an instance was irrelevant, the useful piece of information was if that word existed or not. So we decided that the frequency count for each word would be increased by max 1 for each instance. This approach gave us better definitions.

In our clustering results We got varying cluster sizes. In one case the cluster was only two instances so the context words for that cluster didn't have any nouns whatsoever. So it was not possible to generate the word for the first part of the definition through the vector sum process. So the word 'unknown' was used instead, this approach is justified as the cluster with only 2 instances is definitely an erroneous one and therefore that word sense mostly doesn't exist and hence should not have any definition.        

The basic idea for second part of definition generation of a word is to use the context words of the sentence in which the word is used. We use the Context Free Grammars(CFG) to generate the definition of the words. The CFGs have a set of recursively defined rules(productions) to generate string patterns.

The Context Free Grammar Rules used are:

S 	-> S1
S1 	-> NP VP
NP 	-> Det ADJ N
VP 	-> V NP
Det -> a | the | is
N 	-> Noun words list
V 	-> Verb words list
ADJ -> Adjective words list


We use the Context Free Grammar rules to preserve the syntactic structure of the English grammar. The English grammar has rules such as, Noun phrase should be followed by a Verb Phrase, 'a', 'an', 'the' are Determiners and so on. To preserve such rules and to use them during the sentence generation, we use the Context Free Grammars. The CFG rules are recursively applied to the topic words to generate the definition of the topic words. 

Firstly, we derive the Parts of Speech for each of the topic words and then seperate the words into different sets of Noun/Verb/Adjective based on the dervied Parts of Speech tags.

We iterate over each of the symbols(Terminal symbol) and choose a Non-Terminal or Terminal symbol based on the production rule to generate the definition. 

Examples of Terminal Symbols: S, S1,, NP, VP, Det, N, V, ADJ
Non-Terminal Symbols : Nouns, verbs, adjectives, determiners

Example:

Noun Words	: [('gun', 10), 
			   ('rifle', 6), 
			   ('times', 5), 
			   ('deer', 4), 
			   ('guy', 4)]

Definition  : the young times kill the dead deer 


**********
HOW TO RUN
**********

1) navigate to /scratch/NLP_red
	cd /scratch/NLP_red
2) Start up the python virtual environment. Execute the command
	source bin/activate.csh
3) navigate to the project directory
	cd /scratch/NLP_red/NLP/stage2
4) execute the command
	python cluster_baseline.py

This will run clustering on all of the file in the input directory

********
EXAMPLES
********
-----------------
INPUT FILE FORMAT
-----------------

The input files are taken from the directory: ./input/

The input can be a file either of the following two kinds:

1)An XML with a single Noun or Verb tagged: (single word used in multiple senses)

Example file format:
--------------------

<corpus lang="english">
<lexelt item="line">
<instance id="1">
<answer instance="1" senseid="photograph or videograph a scene or a movie"/>
<context>
, " he says of his subjects. " When they look back, it's more than a portrait of them, it's a moment in place and time. " # As for Leon Borenzstein, he tells people who simply want to look good to go someplace else. But that doesn't stop him from getting clients. # " People are tired of sterile portraits, " he says. " They want something more creative. " # Whether they know it or not, apparently. When San Francisco fashion executive Naomi Mann hired Margretta Mitchell to <head>shoot</head> a family portrait, she asked for a couple of relatively straightforward photographs posed in the living room and garden. But Mitchell happened to snap the five Mann children grouped around a staircase while wearing big grins and goofy hats, and Mann liked it so much she bought that one instead. # " Even if you don't know our family, it speaks to you about who we are, " she says. " It's playful. It's funny. When I saw it, I said,
</context>
</instance>
<instance id="2">
<answer instance="2" senseid="photograph or videograph a scene or a movie"/>
<context>
o, Lesley Gore, fabulous woman, passed away over the weekend. And we also lost Louis Jourdan. Now do you remember a film called " Gigi? " Yes, and also was it " Octopussy, " was he also in that? Yes, he's, he was an amazing Frenchman. We also lost him this weekend. But the good news here is that Naya and Ryan, you both have a lot going on right now. Naya first, what's going on? Well, I am after this going to go and <head>shoot</head> a couple episodes of Lifetime's " Devious Maids. " So I'm excited about that, and it'll be cool to do something different, and all the ladies seem awesome. So I'm really looking forward to it. Yeah, they're all good girls. And what about you, Ryan? You can catch me on " General Hospital " as detective Nathan West. It's February, it's GH Fan February, so if you're a fan of the show, anything you
</context>
</instance>


2) An XML with a Name-conflate pair: (two words conflated into one used in two different senses)

Example file format:
--------------------

<?xml version="1.0" encoding="iso-8859-1" ?>
<corpus lang='english'>

<lexelt item="p_c">

<instance id="1">
<answer instance="1" senseid="pigeon"/>
<context>
 the genome and tissues, as well as the potential parenting, of the band-tailed <head>p_c</head> Patagioenas fasciata. # I've joined forces with a sweep of other interested scientists    fixed. But it is getting prettier. Now traveling under the name of 
</context>
</instance>

<instance id="2">
<answer instance="2" senseid="pigeon"/>
<context>
to beat either in the field or for dollar value -- and splice them into the genome of a stem cell from a common rock <head>p_c</head> # Rock pigeon stem cells containing this doctored genome could be transformed into germ    , and I know they are coming in on my frequency 
</context>
</instance>


******************
OUTPUT FILE FORMAT
******************
The clusters are in the following format.

<corpus lang="english">
<lexelt item="LEXELT">
<instance id="2">
<answer instance="2" senseid="1"/>
<context>
I used to draw a comparison between him and Hindley Earnshaw, and perplex myself to explain satisfactorily why their conduct was so opposite in similar circumstances.  They had both been fond husbands, and were both attached to their children; and I could not see how they shouldn't both have taken the same road, for good or evil.  But, I thought in my mind, Hindley, with apparently the stronger head, has shown himself sadly the worse and the weaker man.  When his ship struck, the captain <head>abandoned</head> his post; and the crew, instead of trying to save her, rushed into riot and confusion, leaving no hope for their luckless vessel.  Linton, on the contrary, displayed the true courage of a loyal and faithful soul: he trusted God; and God comforted him.  One hoped, and the other despaired: they chose their own lots, and were righteously doomed to endure them. But you'll not want to hear my moralising, Mr. Lockwood; you'll judge, as well as I can, all these things: at least, you'll think you will, and that's the same.  The end of Earnshaw was what might have been expected; it followed fast on his sister's: there were scarcely six months between them.  We, at the Grange, never got a very succinct account of his state preceding it; all that I did learn was on occasion of going to aid in the preparations for the funeral.  Mr. Kenneth came to announce the event to my master.
</context>
</instance>

The Confusion matrices and definitions are printed to the stdout.

*********
CITATIONS
*********
1) The main idea for Word2Vec is based on the following paper by Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado and Jeffrey Dean: Distributed representations of words and phrases and their compositionality (https://arxiv.org/pdf/1310.4546.pdf)

@inproceedings{mikolov2013distributed,
  title={Distributed representations of words and phrases and their compositionality},
  author={Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
  booktitle={Advances in neural information processing systems},
  pages={3111--3119},
  year={2013}
}


2)Our main idea for clustering uses Affinity Propogation:(http://www.icmla-conference.org/icmla07/FreyDueckScience07.pdf)

@article{frey2007clustering,
  title={Clustering by passing messages between data points},
  author={Frey, Brendan J and Dueck, Delbert},
  journal={science},
  volume={315},
  number={5814},
  pages={972--976},
  year={2007},
  publisher={American Association for the Advancement of Science}
}





