from src.DefinitionGeneration import Definition
from collections import defaultdict
import sys
import re
from nltk.tag import pos_tag, map_tag
from nltk import word_tokenize
from nltk.util import ngrams
from nltk.collocations import *
from nltk.tokenize import RegexpTokenizer

"""
Performs post processing on the results of the hdp-wsi tool. This includes parsing hdp-wsi's
output and generating definitions. In addition, we experimented with absorbing the smaller
clusters into the larger clusters because the hdp-wsi consistently produced many more
senses than there actually were, but this actually hurt precision for reasons we don't
understand.
"""

# No longer used. See the doc for collapseSmallClusters()
#MIN_CLUSTER_SIZE=6

ctx_assignments = []
ctxes = []
topics = []
# A string which is the name of the word and the first letter of it's pos
# ex: abandon.v
thing = None
target_word = None
pos = None

"""
Converts a senseval2 formatted xml file into the answer key for use by 
the sense cluster scorer script. The answer key is written to
senseclusters_scorer/key
@param f: a string which is a file path to the senseval2 xml file
"""
def makeKey(f):
    global thing
    global target_word
    global pos
    ctx_re = re.compile(r'<instance id="([0-9]*)">.*?<answer.*?senseid="([^"]*)"[^/]*/>.*?<context>(.*?)</context>', re.MULTILINE | re.DOTALL)
    with open(f, 'r') as f_ref:
        buf = f_ref.read()
    mapping = {}
    cur_id = 1
    target, pos = f.split('-', 1)
    pos = re.split('[.-]', pos)[0]
    target_word = target.split('/')[-1]
    thing = target_word + "." + pos[0]
    with open('senseclusters_scorer/key', 'w+') as key_ref:
        for i, (sense_id, sense, ctx) in enumerate(ctx_re.findall(buf)):
            ctxes.append((sense, ctx, sense_id))
            if sense not in mapping:
                mapping[sense] = cur_id
                cur_id += 1
            key_ref.write(thing + " " + thing + "." + str(sense_id) + " " + thing + "." + str(mapping[sense]) + "\n")

"""
Writes the answers generated by hdp-wsi out to a file in the format
that senseclusters_scorer expects. This function should only be called
after readAnswers() has been called. The answer file is written out to
senseclusters_scorer/answers. In addition, also writes out the answers
in a senseval2 formatted xml file, and writes out a definitions file.
Both of these are written to the output directory.
"""
def writeAnswers():
    global pos
    with open('senseclusters_scorer/answers', 'w+') as ans_ref:
        for i, ctx_is in enumerate(ctx_assignments):
            for ctx_i in ctx_is: # TODO: need to change ctx_id to the actual id (not the list index)
                ans_ref.write(thing + " " + thing + "." + str(ctxes[ctx_i][2]) + " " + thing + "." + str(i + 1) + "\n")
    with open('output/' + target_word + '-' + pos + '-assignmnets.xml', 'w+') as f_ref:
        f_ref.write('<corpus lang="english">\n<lexelt item="LEXELT">\n')
        for i, ctx_is in enumerate(ctx_assignments):
            for ctx_i in ctx_is:
                f_ref.write('<instance id="' + str(ctxes[ctx_i][2]) + '">\n')
                f_ref.write('<answer instance="' + str(ctxes[ctx_i][2]) + '" senseid="' + str(i + 1) + '"/>\n')
                f_ref.write('<context>\n')
                f_ref.write(ctxes[ctx_i][1].strip() + '\n')
                f_ref.write('</context>\n</instance>\n')
        f_ref.write('</lexelt>\n</corpus>')
    with open('output/' + target_word + '-' + pos + '.defs', 'w+') as f_ref:
        definition = Definition()
        for i, topic in enumerate(topics):
            if 'noun' in pos or 'verb' in pos:
                f_ref.write(thing + '.' + str(i + 1) + ' definition: ' + definition.generate_Definition(topic, target_word) + '\n')
            else:
                f_ref.write(thing + '.' + str(i + 1) + ' definition: ' + definition.generate_Definition(topic, 'xyz') + '\n')

"""
Reads the topics (which are also the senses) generated by hdp-wsi into
memory.
@param f: a string which is the file path to the topic file generated by
          hdp-wsi
"""
def readTopics(f):
    with open(f, 'r') as f_ref:
        for line in f_ref:
            ctx_assignments.append([])
            topics.append(set(line.split(':', 1)[1].strip().split(' ')))
            topics[-1] = filter(lambda x: x != 'NA', topics[-1])
            topics[-1] = set([topic.split('_')[0] for topic in topics[-1]])

"""
Reads the topic assigments generated by hdp-wsi into memory.
@param f: a string which is the filepath to the topic assignment file
          generated by hdp-wsi
"""
def readAssignments(f):
    with open(f, 'r') as f_ref:
        for line in f_ref:
            best = float(0)
            best_id = None
            data = line.split(' ')
            ctx_id = int(data[1].split('.')[-1]) - 1
            for assignment in data[2:]:
                t_id, prob = assignment.split('/')
                if float(prob) > best:
                    best = float(prob)
                    best_id = int(t_id.split('.')[1]) - 1
            ctx_assignments[best_id].append(ctx_id)

"""
A function that absorbs small clusters (small being defined by MIN_CLUSTER_SIZE) into
a larger cluster which has the most topic words in common with it. We chose not to use
this because it ended up hurting precision, but we are leaving it in because of brandon's
emotional attachment issues.
"""
def collapseSmallClusters():
    for i, topic1 in enumerate(topics):
        if len(ctx_assignments[i]) < MIN_CLUSTER_SIZE:
            print "small cluster: " + str(i)
            best = -1
            best_id = -1
            for j, topic2 in enumerate(topics):
                if topic1 is topic2:
                    continue
                elif len(topic1 & topic2) > best:
                    print "new best: " + str(j)
                    best = len(topic1 & topic2)
                    best_id = j
            print "merging topic " + str(i) + " with topic " + str(best_id)
            ctx_assignments[best_id].extend(ctx_assignments[i])
            topics[best_id] |= topics[i]
            del ctx_assignments[i]
            del topics[i]
            return True
    return False

if __name__ == "__main__":
    if len(sys.argv) < 2:
        print "usage: python postProcessing.py <senseval2-xml-file>"
        sys.exit(1)

    readTopics("hdp-wsi/wsi_output/tm_wsi.topics")
    readAssignments("hdp-wsi/wsi_output/tm_wsi")
    makeKey(sys.argv[1])

    for i, topic in enumerate(topics):
        print "topic " + str(i) + " words: " + ' '.join(word for word in topic)
        #print "senses: "
        #print ', '.join(ctxes[i][0] for i in ctx_assignments[i])

    writeAnswers()

