____________________________________________________________________

		    Team - Red Project Stage I  
	Word sense disambiguation based on co-occurence matrices
____________________________________________________________________

Authors :  	Ankit Anand Gupta 
		Brandon Paulsen 
		Sai Ram Kowshik Vattipally 
		Sandeep Vuppula 
		


*************
CONTRIBUTIONS
*************
Ankit Anand Gupta : Came up with the idea for context free grammar for definition generation. Wrote the code in src/DefinitionGenerator.py in collaboration with Sandeep. Implemented and tried the k-means clustering approach and worked on tf-idf implementation with Sandeep.

Brandon Paulsen : Wrote the majority of the code in writeToHDP.py, postProcessing.py, and corp.py. In addition, figured out how to work with the hdp-wsi tool.

Sai Ram Kowshik Vattipally : Wrote most of the README, including the parts that describe how HDP works, and the instructions for use. He also wrote most of the install.sh and runall.sh. He also installed the Testbed for testing our project on Debian VM.

Sandeep Vuppula : Worked on DefinitionGeneration.py in collaboration with Ankit. Also tested other tools which we decided to not use because hdp-wsi was giving us better results. Implemented and tried the HCA algorithm and worked on tf-idf with Ankit. Came up with N-Gram approach for definition genration which was later found to be relatively ineffective.

********
CONTENTS
********

Contents in the Directory:
* install.sh
* runit.sh
* runall.sh
* README
* RESULTS
* postProcessing.py
* writeToHDP.py
* stopwords.txt (The list of stop words we are excluding)
* hdp-wsi ( Directory that contains HDP word sense induction python implementation)
* senseclusters_scorer
* src
	DefinitionGeneration.py
	corp.py
* output
* input
	abandon-verb-pauls658.xml
	racket-noun-pauls658.xml
	sidewalk-furnace-pauls658.xml
	
	wear-verb-vuppu008.xml
	plot-noun-vuppu008.xml
	television-food-vuppu008.xml

	strike-verb-gupta299.xml
	bat-noun-gupta299.xml
	banana-wall-gupta299.xml

	shoot-verb-vatti001.xml
	date-noun-vatti001.xml
	pigeon-car-vatti001-1.xml

	line-noun-6sense.xml

********************
PROBLEM AND SOLUTION
********************
The problem we are trying to solve is to automatically generate definitions for the different senses of a target word, given several instances of the target word. This also includes discovering how many senses of the target word are present in the given instances. The system takes a XML file containing a list of contexts each of which contain the target word within a <head></head> tag. Each instance is a sentence or a group of sentences that surround the target word which we intend to find the meaning of in the given context. The system we design is intended to group these contexts based on the sense of the target word in that particular context. Each of the resultant cluster will contain contexts where the target word is used in same sense. This step is called clustering.

To perform clustering, we use a pre-existing tool called hdp-wsi. hdp-wsi is a tool designed to perform word sense induction, including discovering the number of senses of the target word, so the bulk of the work is done by this tool. The main things we made for this project were scripts to convert a senseval2 file into a format the can be processed by hdp-wsi, and a definition generator. hdp-wsi does not remove stopwords (though it seems like it should), so we also wrote scripts to tokenize the contexts and remove stopwords. 

The approach taken by hdp-wsi is based on topic modeling, so in addition to clustering the target word and contexts into related senses, hdp-wsi gives us several "topic" words associated with each cluster. From these topic words we generate sentences which are the definitions of the target word in each cluster. The core of our definition generation system is a context free grammar created by ankit which defines a simple sentence structure. We then take the topic words from hdp-wsi, POS tag them, and then generate random sentences using our context free grammar. These sentences are then joined together with conjuctions until we have at least a 10 word definition.

The input file may be of 2 kinds. The first one has only one target word which is either a noun or verb which has multiple senses. The second kind is where there are contexts that uses two different words that are conflated into a single word. In the first kind, we try to cluster the contexts based on various senses the target word we have. In the second one, we try to assign a sense to the conflated word and try to group the contexts into clusters such that each cluster might have same sense as each word that the conflated word is formed from. Note that we might generate more or less clusters than the number of words used for conflating the word.

___________________________________
Point-by-point outline of execution
___________________________________

The main entry point is runit.sh. This script mainly serves to call other scripts and doesn't do any real work besides clearing out the results from the previous run. This function is called with a single senseval2 formatted xml file as the first argument like so:
./runit.sh input/bat-noun-gupta299.xml
Besides clearing the last run, the first thing done by runit.sh is to call writeToHDP.py with the xml file as it's only argument. The purpose of writeToHDP.py is to convert the xml file into a format the hdp-wsi tool can process. To do this, we need to know what the target word is, so we extract it from the filename as described in COM1. In the case of a name-conflate file, we need to set the target word to xyz, so we determine if the file is a name-conflate pair by checking if the word "noun" or "verb" appears in the file name as described in COM2. If one of these is not present, the file is for a name-conflate pair. The need for the target word is described in the next paragraph. The next significant processing is done at the location marked by COM3 using the function getCtxes(), which is labeled as FUN1 in writeToHDP.py. This function takes the file path to the xml file, and returns two lists which are connected by index. The first is a list of the raw document texts, and the second is a list of senses, where the sense of document[i] corresponds to sense[i]. The sense list is a relic of our testing phase, but it was left in because it may be useful in the future. The for loop marked at COM3 processes each raw document in the first list returned by getCtxes() with the tokenize() function.
The tokenize() function is imported from src/corp.py, and is labeled as FUN2 in said file. This function takes three parameters, which are documented in src/corp.py. The ctx_len parameter is a relic from when we experimented with only including a specified window surrounding the target word, however we found that including all context produced the best results. We now set this parameter to an arbitrarily high number (500) to include all context. The description of how this function works, and why the target word is a necessary parameter are also described in FUN2.
The final action of writeToHDP.py is to take the corp produced by the previous for loop, and write it out into the hdp-wsi input directory in a format that the tool can read. This is done by a function call to writeCorpToHDPWSI(), marked as FUN3 in writeToHDP.py. See the FUN3 for parameter documentation. This function takes the given corpus (a list of tokenized documents), joins the tokens back together into a single string, and writes the string out to the input file for hdp-wsi in the proper format. The input format for a single target word is a single file where the file name is the target word and the first letter of the part-of-speech, separated by a period (ex: "bat.n" if the target word is bat).
After the input file is written, the main entry point for hdp-wsi is hdp-wsi/run_wsi.sh. This script does not do anything interesting with regards to WSI. It simply takes the input file we just generated, and moves it another input directory for processing by the script hdp-wsi/topicmodelling/run_topicmodel.sh. run_topicmodel.sh is similar to runit.sh in that it mostly calls other python scripts.
The first script called in run_topicmodel.sh, MakeWordStream.py, converts the input file (which is still the same file that was generated by our scripts) into a word stream format, in which each line is either a token for a document, or a document divider. Basically it changes the way that our documents are represented for easier processing. 
Next, run_topicmodel.sh calls a another python script, AddContextWord.py, which adds positional information to each document in the newly created word stram. For each document in the word stream, it takes the six closest tokens to the target word, and then it adds six more tokens to the word stream with a "_#x" appended to the tokem, where x is the distance from the target word. The goal of this is to create "special" tokens for the tokens that appear close to the target word. These new tokens are simply appended to the end of the document's word stream since postion won't matter when we run HDP. 
The next script, MakeVocab.py, determines which tokens for each document we will keep. We decide to keep a token if its overall count across all document is greater than the "voc_minfreq" parameter define in run_topicmodel.sh. The throwaway tokens aren't removed from the word stream in this step though. This script only outputs the words to keep, or vocab, in topicmodeloutput/vocabs.txt.
The final script before we run HDP, ConvertDataToHDPFormat.py, converts the input word stream into the format that the hdp tool expects. In addition, it removes tokens from the documents that do not appear in vocabs.txt. This script converts each document into an unordered bag of tokens, where each token is replaced with a unique identification number. The assignment of identification numbers to tokens is arbitrary. The script just reads in each token from vocabs.txt, and sequentially assigns them ids. Then each document is converted into a dictionary where the key is the token id, and the value is the number of times that token occurs in the document. Ex:
{ 102: 1, 5: 6, 82: 3, 55: 2 }
Is a document with the tokens 102, 5, 82, and 55, and they occur 1, 6, 3, and 2 times respectively. After this conversion is performed, the HDP is run on this file. The clustering algorithm is described below. In addition to the input file just described, the HDP process takes as arguments an alpha and gamma value which are also described in the section below. The hdp process outputs two file of importance: a topics file, and a topic assignmen file.
The topics file, which is output in mode-topics.dat, is a new-line seperated list of topics, where each line is a list of token ids which represent the topic words for the given topic. These topics represent the clusters to which each document will be assigned. The words assigned to each topic are also the words used to generate definitions for each cluster.
The topic assignmen file, which is output in mode-word-assignments.dat, is also a new-line seperated list of topic assignmens where each line has the format:
<doc-id> <word-id> <topic-id> <table-id>
In topic modelling, each document can have multiple topics, so, instead of assigning each document a topic, each word in a document is assigned a topic. So a line from the topic assignmens file means that the word represented by word-id found in the document associated with doc-id is assigned to the topic topic-id. Thus there is a additional processing to do on this output to assign each documnet to a cluster. The last column is irrelevant for the purpose WSI.
The assignment of a document to a cluster (or topic) is done in the next script of run_topicmodel.sh, called CalcHDPTopics.py. The assignmen is done by creating a probability distribution of all the topics found in a document. The topic with the largest portion of the distribution is taken to be the document's cluster assignmen. For example, if a document has 20 tokens in it, and 5 of those tokens were assigned to topic 1, and 15 were assigned to topic 2, the assignmen for this document would be topic 2.
The final scripts in run_topicmodel.sh are utility scripts that convert the word ids back into human readable words, and move the resulting files into hdp-wsi/wsi-output. The two files we are interested in are the human-readable topic word files (located in hdp-wsi/wsi_output/tm_wsi.topics) which contains the words associated with each topic, and the topic (or cluster) assignments (located in hdp-wsi/wsi_output/tm_wsi).
After run_wsi.sh finishes, the main script executes postProcessing.py which performs some additional clustering, calls utility functions to generate definitions, and converts the output of hdp-wsi to the required format for this assignmen. The first two functions calls (marked by COM4 in postProcessing.py) read tm_wsi.topics and tm_wsi respectively into memory. readTopics() (marked by FUN4) simply reads the words on a single line into a list and then associates that list with a topic id. readAssignments() (marked by FUN5) reads the topic assignmens from tm_wsi, however hdp-wsi doesn't always output a single topic assignment per document, so some documens have multiple topics. We handle this by iterating through all of the assignments, and taking the topic assignmen with the highest probability as pointed out in COM6. The third function call to makeKey() (marked FUN6) generates the oracle file for sense_clustersscorer, which converts the original xml document into the format the scorer expects.
In addition to hdp-wsi, we perform some of our post-clustering with the next two while loops. We noticed that hdp-wsi would often create small clusters of less than six documents. Since the actual clusters are generally more than 20, it was in our benefit to combine these small clusters (small being defined by the variable MIN_CLUSTER_SIZE) with larger clusters. When we find a cluster with a size less than MIN_CLUSTER_SIZE, we combine it with the other cluster that has the most topic words in common with it. The logic for this is encoded in the first while loop by repeated calling collapseSmallClusters() until it returns false (meaning there is no more collapsing to be done). Notice that we also enforce the constraint that we must have at least two clusters (otherwise this wouldn't be word sense disambiguation!).
In addition, we also noticed that hdp-wsi would break up an otherwise good cluster into two or more clusters (thus halving our accuracy for that cluster). We also noticed that these seperated clusters would have several topic words in common, so we apply another cluster collapsing method that combines two clusters if they have at least CLUSTER_SIM topic words in common. The logic for this is encoded similarly in the second while loop, which repeatedly calls collapseSimilarClusters() until it returns false.
The final function call is writeAnswers(), which writes out our final document assignments in two different formats: one for the sense cluster scorer, and one in senseval2 format. In addition, this function calls generate_Definition() on each set of topic words, which generates definitions for the clusters. The execution of this is outlined in the below section titled Definition Generation.
__________
Clustering
__________

We first parse the XML input file and extract each of the contexts seperately. We then tokenize each of the context and convert all the tokens to lower case alphabet. The tokenizer breaks certain words into contractions(For example "isn't" becomes "is + n't"). We then remove a selected list of stop words and words that have punctuations and numbers in them ( eg: n't, T2000) from the contexts because we have found that the presence of these words leads to poorer clustering. Then the resultant contexts as given as an input to the Hdp-wsi(Hierarchical Dirichlet process-Word sense Induction) tool in a suitable format. ***. The hdp-wsi tool extracts topics(senses) from the contexts and assigns descriptor words to each topic. Then we assign each context to a topic based on how well the context matches the descriptor words that belong to a particular topic. Then each of these topics become our senses that we extract from the given contexts.



Hierarchical Dirichlet process (HDP) is a algorithm used for clustering already grouped data. It takes N number of groups and clusters them into K groups. It is based on Dirichlet process. The number of groups is unknown prior to running the algorithm and is extracted from the data. We assume that each of our context is a group of its own and give them as an input to the HDP algorithm. The HDP algorithm clusters by finding similarity in latent structure between each of these groups. 

The Dirichlet process can be represented as 

DP(a_0, G_0)

*Alpha(a_0) is the scaling parameter and alpha >= 0. In simple terms, probability of a new cluster being generated is proportional to the a_0.

*Gamma(G_0) is the base probability measure. In other words, We are assuming that our data follows a base probability distribution G_0.
The authors use default settings of Base probability measure, Gamma = 0.1 and scaling parameter,alpha = 1.0 in the current implementaion. These settings were considered “vague” in the initial implementaion of hdp-wsi in Teh et al. (2006)( in which both alpha and gamma were set to 0.1 as default parameter settings). These parameters were later tested in Lau et al. (2012) and the model was found to be robust under various parameter settings. Hence our best guess is that the parameters won't affect the results a lot if there is enough data for our system to train on.

But given small size of our data, we found that the default parameters for the hdp-wsi consistently produced too many clusters. To try and combat this, we lowered the alpha value. Overall this improved our precision, however it drastically reduced the precision for the sidewalk-furnace conflate pair. We have yet to understand why. 

In addition, the implementation of hdp that is used by the authors of hdp-wsi uses a random seed value which makes the clustering and topic word generation non-deterministic (we get different results each time we run; We've seen the precision vary by up to 15%). This is not ideal for testing modifications, so we fixed the random seed value to 1. This value was chosen arbitrarily, and it could have been any value. The important thing is that we set it to 1, and then did not change it.

The original implementation of Hdp-wsi (python module) is tested on SemEval-2007 and SemEval-2010 WSI datasets. The datasets are similar in structure to the data we are using. Similar to the case in the word sense induction part of the paper, we consider each sentence or a given group of sentences containing the target word as a single "document" and there was usually at least 1 sentence before and after the sentence containing the target word. The dataset for WSI in the hdp-wsi paper was the SemEval-2010 data set which consists of about 160,000 instances of about 100 target words. This is an average of 1600 instances per word, where as our data has exactly 100 instances per word. We think that the relative poor performance of our sense clustering algorithm is due to the smaller size of our dataset ( by a factor of 16) compared to the dataset used to test the hdp-wsi implementaion by the authors of the hdp-wsi paper.
Also its interesting to note that although the test instances were the same, the authors used additional data extracted from English ukWaC(a web corpus) as training data saying that the original dataset is too small to induce any meaningful senses. They do the topic modelling based on a lot more training data than the said 100 instances but finally run the test only on the "official" 100 instances.
_____________________
Definition generation
_____________________

The basic idea for definition generation of a word is to use the context words of the sentence in which the word is used. To generate the definition for each cluster, we take a set of topic words in each cluster. The topic words are generated by the hdp program. We can consider the topic words as a set of most frequently occurring words in each cluster. Also care is taken not to include the target word while generating it's meaning. We use the Context Free Grammars(CFG) to generate the definition of the words. The CFGs have a set of recursively defined rules(productions) to generate string patterns.
The Context Free Grammar Rules used are:

S 	 -> S1 CONJ S2
S1 	 -> NP VP
S2 	 -> NP VP
NP 	 -> Det N
VP 	 -> V PRO ADJ NP
PRO  -> with | to
Det  -> a | the | is
N 	 -> Noun words list
V 	 -> Verb words list
ADJ  -> Adjective words list
CONJ -> and

We use the Context Free Grammar rules to preserve the syntactic structure of the English grammar. The English grammar has rules such as, Noun phrase should be followed by a Verb Phrase, 'a', 'an', 'the' are Determiners and so on. To preserve such rules and to use them during the sentence generation, we use the Context Free Grammars. The CFG rules are recursively applied to the topic words to generate the definition of the topic words. 

Firstly, we derive the Parts of Speech for each of the topic words and then seperate the words into different sets of Noun/Verb/Adjective based on the dervied Parts of Speech tags.

We iterate over each of the symbols(Terminal symbol) and choose a Non-Terminal or Terminal symbol based on the production rule to generate the definition. 

Examples of Terminal Symbols: S, S1, S2, NP, VP, Det, N, V, ADJ, PRO, CONJ
Non-Terminal Symbols : Nouns, verbs, adjectives, pronouns, conjunctions, determiners

Example:
Topic words	:  shoot woman love look movie director part lot money film
POS_TAGs	: [ ('woman', u'NOUN'), 
				('love', u'VERB'), 
				('look', u'NOUN'), 
				('movie', u'NOUN'), 
				('director', u'NOUN'), 
				('part', u'NOUN'), 
				('lot', u'NOUN'), 
				('money', u'NOUN'), 
				('film', u'NOUN') ]
Definition  : money love with a movie and a director love with is lot 


Point-by-Point Code Reference:
------------------------------
Section I : get_Noun_Verb()
			The function is used to seperate the Nouns, Verbs and Adjectives in the given set of topic words.
			We use the Parts of Speech Tagger from the Natural Language Toolkit to tag the POS for each word in the set of topic words.
			It takes a set of topic words, assigns respective POS tags to each of those words. And then for each tag(Noun/Verb/Adjective), it forms a string of  words seperated by '|'
Section II: cfg_rule()
			The function is used to map the Context Free Grammar production rules for the english grammar to python representation.
			It takes a rule of the form "PRO  -> with | to" and splits the rules on the right hand side based on "|". Then it appends the rules present on the right side to the rule present on the left.
Section III: gen_def()
			The function is used to generate the definition of a sentence recursively using the CFG rules. It chooses a production rule randomly from the list and then recursively gen_def() function is called. In order to avoid the same word being picked twice, we remove the word from the list of Nouns/Verbs/Adjectives once it is used.
Section IV: generate_Definition()
			This is the main class which drives the definition generation. First it takes a list of topic words and removes the target word from the list. Later it retrieves the Noun/Verb/Adjective words in the form of concatenated string separated by "|".  Then it takes the production rules and maps them as Array item -> set of tuples. Then gen_def() function proceeds to generate the definition based on the set of topic words.


**********
HOW TO RUN
**********

1) In terminal, navigate to the directory of project.
2) Run install.sh as "./install.sh". This simply installs all the necessary packages for running
3) To run word sense induction on a single senseval2 xml file, simply execute:
	./runit.sh <senseval2-xml-file>

Eg: To run our project on the file "date-noun-vatti001.xml", run as 
	./runit.sh input/date-noun-vatti001.xml

- You can also run the tool on every file in the input directory with the following command:
	./runall.sh

4) The reults of cluster creation, assignmen of contexts to clusters, and the definitions generated for each cluster will be written in files in the output directory. See the section on output file format for an explanation of the format


********
EXAMPLES
********
-----------------
INPUT FILE FORMAT
-----------------

The input files are taken from the directory: ./input/

The input can be a file either of the following two kinds:

1)An XML with a single Noun or Verb tagged: (single word used in multiple senses)

Example file format:
--------------------

<corpus lang="english">
<lexelt item="line">
<instance id="1">
<answer instance="1" senseid="photograph or videograph a scene or a movie"/>
<context>
, " he says of his subjects. " When they look back, it's more than a portrait of them, it's a moment in place and time. " # As for Leon Borenzstein, he tells people who simply want to look good to go someplace else. But that doesn't stop him from getting clients. # " People are tired of sterile portraits, " he says. " They want something more creative. " # Whether they know it or not, apparently. When San Francisco fashion executive Naomi Mann hired Margretta Mitchell to <head>shoot</head> a family portrait, she asked for a couple of relatively straightforward photographs posed in the living room and garden. But Mitchell happened to snap the five Mann children grouped around a staircase while wearing big grins and goofy hats, and Mann liked it so much she bought that one instead. # " Even if you don't know our family, it speaks to you about who we are, " she says. " It's playful. It's funny. When I saw it, I said,
</context>
</instance>
<instance id="2">
<answer instance="2" senseid="photograph or videograph a scene or a movie"/>
<context>
o, Lesley Gore, fabulous woman, passed away over the weekend. And we also lost Louis Jourdan. Now do you remember a film called " Gigi? " Yes, and also was it " Octopussy, " was he also in that? Yes, he's, he was an amazing Frenchman. We also lost him this weekend. But the good news here is that Naya and Ryan, you both have a lot going on right now. Naya first, what's going on? Well, I am after this going to go and <head>shoot</head> a couple episodes of Lifetime's " Devious Maids. " So I'm excited about that, and it'll be cool to do something different, and all the ladies seem awesome. So I'm really looking forward to it. Yeah, they're all good girls. And what about you, Ryan? You can catch me on " General Hospital " as detective Nathan West. It's February, it's GH Fan February, so if you're a fan of the show, anything you
</context>
</instance>


2) An XML with a Name-conflate pair: (two words conflated into one used in two different senses)


Example file format:
--------------------

<?xml version="1.0" encoding="iso-8859-1" ?>
<corpus lang='english'>

<lexelt item="p_c">

<instance id="1">
<answer instance="1" senseid="pigeon"/>
<context>
 the genome and tissues, as well as the potential parenting, of the band-tailed <head>p_c</head> Patagioenas fasciata. # I've joined forces with a sweep of other interested scientists    fixed. But it is getting prettier. Now traveling under the name of 
</context>
</instance>

<instance id="2">
<answer instance="2" senseid="pigeon"/>
<context>
to beat either in the field or for dollar value -- and splice them into the genome of a stem cell from a common rock <head>p_c</head> # Rock pigeon stem cells containing this doctored genome could be transformed into germ    , and I know they are coming in on my frequency 
</context>
</instance>


******************
OUTPUT FILE FORMAT
******************

The output files are saved in the following directory: ./output/

runit.sh will generate two output files for a given input file: one that shows the assignments of contexts into clusters, and one that shows the definition for each cluster. The assignment file should be named <target-word>-<pos>-assignments.xml. This is a senseval2 formatted file which is basically a list of instances where each instance has a context, and a cluster ID. The output file is identical to the input file, except that the senseid attributes for each instance have been replaced with a number which denotes membership to a specific cluster. The number is the ID of the cluster that the context belongs to. In addition, the instance's retain the same ID as the had in the original input file. In the following example, the instance with id=2 in the input file has been assigned to cluster 1.

<corpus lang="english">
<lexelt item="LEXELT">
<instance id="2">
<answer instance="2" senseid="1"/>
<context>
I used to draw a comparison between him and Hindley Earnshaw, and perplex myself to explain satisfactorily why their conduct was so opposite in similar circumstances.  They had both been fond husbands, and were both attached to their children; and I could not see how they shouldn't both have taken the same road, for good or evil.  But, I thought in my mind, Hindley, with apparently the stronger head, has shown himself sadly the worse and the weaker man.  When his ship struck, the captain <head>abandoned</head> his post; and the crew, instead of trying to save her, rushed into riot and confusion, leaving no hope for their luckless vessel.  Linton, on the contrary, displayed the true courage of a loyal and faithful soul: he trusted God; and God comforted him.  One hoped, and the other despaired: they chose their own lots, and were righteously doomed to endure them. But you'll not want to hear my moralising, Mr. Lockwood; you'll judge, as well as I can, all these things: at least, you'll think you will, and that's the same.  The end of Earnshaw was what might have been expected; it followed fast on his sister's: there were scarcely six months between them.  We, at the Grange, never got a very succinct account of his state preceding it; all that I did learn was on occasion of going to aid in the preparations for the funeral.  Mr. Kenneth came to announce the event to my master.
</context>
</instance>

The defintions file should be named <target-word>-<pos>.defs. This file contains the definition generated for each cluster. In the following example, cluster 1 has the definition "a land england with a hands and a death want with is john", and so on. The number next to the word represents the cluster ID, so going back to the previous example, the definition of the sense of abandon being used in instance 2 corresponds to the previously mentioned definition. 

abandon.v.1 definition: a land england with a hands and a death want with is john 
abandon.v.2 definition: the part england to a place and the death took to the hands 
abandon.v.3 definition: is death found with is home and is case want with is place 
abandon.v.4 definition: the god found with is ship and a hands want to is ship 
abandon.v.5 definition: the house want to a case and is place thought with a death 
abandon.v.6 definition: is course brought with is sea and the john left to a mind 
abandon.v.7 definition: a mind england with the board and is things found to a god 
abandon.v.8 definition: the ship took with is death and a children brought with a case 
abandon.v.9 definition: is place told to the children and a course found to a house 
abandon.v.10 definition: is water found with a vessel and a board hopes with able is board 

*********
CITATIONS
*********

1)Our main idea for clustering uses HDP (Hierarchical Dirichlet process), an algorithm developed by Yee Whye Teh, Michael I. Jordan, Matthew J. Beal and David Blei:
 Hierarchical dirichlet processes
(http://www.gatsby.ucl.ac.uk/~ywteh/research/npbayes/jasa2006.pdf)

@article{teh2012hierarchical,
  title={Hierarchical dirichlet processes},
  author={Teh, Yee Whye and Jordan, Michael I and Beal, Matthew J and Blei, David M},
  journal={Journal of the american statistical association},
  year={2012},
  publisher={Taylor \& Francis}
}


2) Our implementaion uses a python package developed by Jey Han Lau, Paul Cook, Diana McCarthy, David Newman and Timothy Baldwin that that utilizes the HDP implementation of the previous authors to perform WSI: 
Word sense induction for novel sense detection
(http://www.ics.uci.edu/~newman/pubs/eacl2012.pdf)

@inproceedings{lau2012word,
  title={Word sense induction for novel sense detection},
  author={Lau, Jey Han and Cook, Paul and McCarthy, Diana and Newman, David and Baldwin, Timothy},
  booktitle={Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics},
  pages={591--601},
  year={2012},
  organization={Association for Computational Linguistics}
}




