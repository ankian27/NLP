____________________________________________________________________

		    Team - Red Project Stage I  
	Word sense disambiguation based on co-occurence matrices
____________________________________________________________________

Authors :  	Ankit Anand Gupta -
		Brandon Paulsen -
		Sai Ram Kowshik Vattipally -
		Sandeep Vuppula -
		


*************
CONTRIBUTIONS
*************
Ankit Anand Gupta : Came up with the idea for context free grammar for definition generation. Wrote the code in src/DefinitionGenerator.py in collaboration with Sandeep.

Brandon Paulsen : Wrote the majority of the code in writeToHDP.py, postProcessing.py, and corp.py. In addition, figured out how to work with the hdp-wsi tool.

Sai Ram Kowshik Vattipally : Wrote most of the README, including the parts that describe how HDP works, and the instructions for use.

Sandeep Vuppula : Worked with Ankit on DefinitionGeneration.py. Also tested other tools which we decided to not use because hdp-wsi was giving us better results.

********
CONTENTS
********

Contents in the Directory:
* install.sh
* runit.sh
* runall.sh
* README
* RESULTS
* postProcessing.py
* writeToHDP.py
* stopwords.txt (The list of stop words we are excluding)
* hdp-wsi ( Directory that contains HDP word sense induction python implementation)
* senseclusters_scorer
* src
	DefinitionGeneration.py
	corp.py
* output
* input
	abandon-verb-pauls658.xml
	racket-noun-pauls658.xml
	sidewalk-furnace-pauls658.xml
	
	wear-verb-vuppu008.xml
	plot-noun-vuppu008.xml
	television-food-vuppu008.xml

	strike-verb-gupta299.xml
	bat-noun-gupta299.xml
	banana-wall-gupta299.xml

	shoot-verb-vatti001.xml
	date-noun-vatti001.xml
	pigeon-car-vatti001-1.xml

********************
PROBLEM AND SOLUTION
********************
The problem we are trying to solve is creating a dictionary from raw data. The system takes a XML file containing a list of contexts each of which contain a target word within a <head></head> tag. Each instance is a sentence or a group of sentences that surround the target word which we intend to find the meaning of in the given context. The system we design is intended to group these contexts based on the sense of the target word in that particular context. Each of the resultant cluster will contain contexts where the target word is used in same sense. This step is called clustering.

To perform clustering, we use a pre-existing tool called hdp-wsi. The approach taken by hdp-wsi is based on topic modeling, so in addition to clustering the target word and contexts into related senses, hdp-wsi gives us several "topic" words associated with each cluster. From these topic words we generate sentences which are the definitions of the target word in each cluster.

The input file may be of 2 kinds. The first one has only one target word which is either a noun or verb which has multiple senses. The second kind is where there are contexts that uses two different words that are conflated into a single word. In the first kind, we try to cluster the contexts based on various senses the target word we have. In the second one, we try to assign a sense to the conflated word and try to group the contexts into clusters such that each cluster might have same sense as each word that the conflated word is formed from. Note that we might generate more or less clusters than the number of words used for conflating the word.
__________
Clustering
__________

We first parse the XML input file and extract each of the contexts seperately. We then tokenize each of the context and convert all the tokens to lower case alphabet. The tokenizer breaks certain words into contractions(For example "isn't" becomes "is + n't"). We then remove a selected list of stop words and words that have punctuations and numbers in them ( eg: n't, T2000) from the contexts because we have found that the presence of these words leads to poorer clustering. Then the resultant contexts as given as an input to the Hdp-wsi(Hierarchical Dirichlet process-Word sense Induction) tool in a suitable format. ***. The hdp-wsi tool extracts topics(senses) from the contexts and assigns descriptor words to each topic. Then we assign each context to a topic based on how well the context matches the descriptor words that belong to a particular topic. Then each of these topics become our senses that we extract from the given contexts.



Hierarchical Dirichlet process (HDP) is a algorithm used for clustering already grouped data. It takes N number of groups and clusters them into K groups. It is based on Dirichlet process. The number of groups is unknown prior to running the algorithm and is extracted from the data. We assume that each of our context is a group of its own and give them as an input to the HDP algorithm. The HDP algorithm clusters by finding similarity in latent structure between each of these groups. 

The Dirichlet process can be represented as 

DP(a_0, G_0)

*Alpha(a_0) is the scaling parameter and alpha >= 0. In simple terms, probability of a new cluster being generated is proportional to the a_0.

*Gamma(G_0) is the base probability measure. In other words, G_0 is the probability that a context goes into a newly generated cluster.

We found that the default parameters for the hdp-wsi consistently produced too many clusters. To try and combat this, we lowered the alpha value. Overall this improved our precision, however it drastically reduced the precision for the sidewalk-furnace conflate pair. We have yet to understand why.


_____________________
Definition generation
_____________________

The basic idea for definition generation of a word is to use the context words of the sentence in which the word is used. To generate the definition for each cluster, we take a set of topic words in each cluster. The topic words are generated by the hdp program. We can consider the topic words as a set of most frequently occurring words in each cluster. Also care is taken not to include the target word while generating it's meaning. We use the Context Free Grammars(CFG) to generate the definition of the words. The CFGs have a set of recursively defined rules(productions) to generate string patterns.
The Context Free Grammar Rules used are:

S 	 -> S1 CONJ S2
S1 	 -> NP VP
S2 	 -> NP VP
NP 	 -> Det N
VP 	 -> V PRO ADJ NP
PRO  -> with | to
Det  -> a | the | is
N 	 -> Noun words list
V 	 -> Verb words list
ADJ  -> Adjective words list
CONJ -> and

We use the Context Free Grammar rules to preserve the syntactic structure of the English grammar. The English grammar has rules such as, Noun phrase should be followed by a Verb Phrase, 'a', 'an', 'the' are Determiners and so on. To preserve such rules and to use them during the sentence generation, we use the Context Free Grammars. The CFG rules are recursively applied to the topic words to generate the definition of the topic words. 

Firstly, we derive the Parts of Speech for each of the topic words and then seperate the words into different sets of Noun/Verb/Adjective based on the dervied Parts of Speech tags.

We iterate over each of the symbols(Terminal symbol) and choose a Non-Terminal or Terminal symbol based on the production rule to generate the definition. 

Examples of Terminal Symbols: S, S1, S2, NP, VP, Det, N, V, ADJ, PRO, CONJ
Non-Terminal Symbols : Nouns, verbs, adjectives, pronouns, conjunctions, determiners

Example:
Topic words	:  shoot woman love look movie director part lot money film
POS_TAGs	: [ ('woman', u'NOUN'), 
				('love', u'VERB'), 
				('look', u'NOUN'), 
				('movie', u'NOUN'), 
				('director', u'NOUN'), 
				('part', u'NOUN'), 
				('lot', u'NOUN'), 
				('money', u'NOUN'), 
				('film', u'NOUN') ]
Definition  : money love with a movie and a director love with is lot 


**********
HOW TO RUN
**********

1) In terminal, navigate to the directory of project.
2) Run install.sh as "./install.sh". This simply installs all the necessary packages for running
3) To run word sense induction on a single senseval2 xml file, simply execute:
	./runit.sh <senseval2-xml-file>

Eg: To run our project on the file "date-noun-vatti001.xml", run as 
	./runit.sh input/date-noun-vatti001.xml

- You can also run the tool on every file in the input directory with the following command:
	./runall.sh

4) The reults of cluster creation, assigment of contexts to clusters, and the definitions generated for each cluster will be written in files in the output directory. See the section on output file format for an explanation of the format


********
EXAMPLES
********
-----------------
INPUT FILE FORMAT
-----------------
The input can be a file either of the following two kinds:

1)An XML with a single Noun or Verb tagged: (single word used in multiple senses)

Example file format:
--------------------

<corpus lang="english">
<lexelt item="line">
<instance id="1">
<answer instance="1" senseid="photograph or videograph a scene or a movie"/>
<context>
, " he says of his subjects. " When they look back, it's more than a portrait of them, it's a moment in place and time. " # As for Leon Borenzstein, he tells people who simply want to look good to go someplace else. But that doesn't stop him from getting clients. # " People are tired of sterile portraits, " he says. " They want something more creative. " # Whether they know it or not, apparently. When San Francisco fashion executive Naomi Mann hired Margretta Mitchell to <head>shoot</head> a family portrait, she asked for a couple of relatively straightforward photographs posed in the living room and garden. But Mitchell happened to snap the five Mann children grouped around a staircase while wearing big grins and goofy hats, and Mann liked it so much she bought that one instead. # " Even if you don't know our family, it speaks to you about who we are, " she says. " It's playful. It's funny. When I saw it, I said,
</context>
</instance>
<instance id="2">
<answer instance="2" senseid="photograph or videograph a scene or a movie"/>
<context>
o, Lesley Gore, fabulous woman, passed away over the weekend. And we also lost Louis Jourdan. Now do you remember a film called " Gigi? " Yes, and also was it " Octopussy, " was he also in that? Yes, he's, he was an amazing Frenchman. We also lost him this weekend. But the good news here is that Naya and Ryan, you both have a lot going on right now. Naya first, what's going on? Well, I am after this going to go and <head>shoot</head> a couple episodes of Lifetime's " Devious Maids. " So I'm excited about that, and it'll be cool to do something different, and all the ladies seem awesome. So I'm really looking forward to it. Yeah, they're all good girls. And what about you, Ryan? You can catch me on " General Hospital " as detective Nathan West. It's February, it's GH Fan February, so if you're a fan of the show, anything you
</context>
</instance>


2) An XML with a Name-conflate pair: (two words conflated into one used in two different senses)


Example file format:
--------------------

<?xml version="1.0" encoding="iso-8859-1" ?>
<corpus lang='english'>

<lexelt item="p_c">

<instance id="1">
<answer instance="1" senseid="pigeon"/>
<context>
 the genome and tissues, as well as the potential parenting, of the band-tailed <head>p_c</head> Patagioenas fasciata. # I've joined forces with a sweep of other interested scientists    fixed. But it is getting prettier. Now traveling under the name of 
</context>
</instance>

<instance id="2">
<answer instance="2" senseid="pigeon"/>
<context>
to beat either in the field or for dollar value -- and splice them into the genome of a stem cell from a common rock <head>p_c</head> # Rock pigeon stem cells containing this doctored genome could be transformed into germ    , and I know they are coming in on my frequency 
</context>
</instance>


******************
OUTPUT FILE FORMAT
******************

runit.sh will generate two output files for a given input file: one that shows the assignments of contexts into clusters, and one that shows the definition for each cluster. The assignment file should be named <target-word>-<pos>-assignments.xml. This is a senseval2 formatted file which is basically a list of instances where each instance has a context, and a cluster ID. The output file is identical to the input file, except that the senseid attributes for each instance have been replaced with a number which denotes membership to a specific cluster. The number is the ID of the cluster that the context belongs to. In addition, the instance's retain the same ID as the had in the original input file. In the following example, the instance with id=2 in the input file has been assigned to cluster 1.

<corpus lang="english">
<lexelt item="LEXELT">
<instance id="2">
<answer instance="2" senseid="1"/>
<context>
I used to draw a comparison between him and Hindley Earnshaw, and perplex myself to explain satisfactorily why their conduct was so opposite in similar circumstances.  They had both been fond husbands, and were both attached to their children; and I could not see how they shouldn't both have taken the same road, for good or evil.  But, I thought in my mind, Hindley, with apparently the stronger head, has shown himself sadly the worse and the weaker man.  When his ship struck, the captain <head>abandoned</head> his post; and the crew, instead of trying to save her, rushed into riot and confusion, leaving no hope for their luckless vessel.  Linton, on the contrary, displayed the true courage of a loyal and faithful soul: he trusted God; and God comforted him.  One hoped, and the other despaired: they chose their own lots, and were righteously doomed to endure them. But you'll not want to hear my moralising, Mr. Lockwood; you'll judge, as well as I can, all these things: at least, you'll think you will, and that's the same.  The end of Earnshaw was what might have been expected; it followed fast on his sister's: there were scarcely six months between them.  We, at the Grange, never got a very succinct account of his state preceding it; all that I did learn was on occasion of going to aid in the preparations for the funeral.  Mr. Kenneth came to announce the event to my master.
</context>
</instance>

The defintions file should be named <target-word>-<pos>.defs. This file contains the definition generated for each cluster. In the following example, cluster 1 has the definition "a land england with a hands and a death want with is john", and so on. The number next to the word represents the cluster ID, so going back to the previous example, the definition of the sense of abandon being used in instance 2 corresponds to the previously mentioned definition. 

abandon.v.1 definition: a land england with a hands and a death want with is john 
abandon.v.2 definition: the part england to a place and the death took to the hands 
abandon.v.3 definition: is death found with is home and is case want with is place 
abandon.v.4 definition: the god found with is ship and a hands want to is ship 
abandon.v.5 definition: the house want to a case and is place thought with a death 
abandon.v.6 definition: is course brought with is sea and the john left to a mind 
abandon.v.7 definition: a mind england with the board and is things found to a god 
abandon.v.8 definition: the ship took with is death and a children brought with a case 
abandon.v.9 definition: is place told to the children and a course found to a house 
abandon.v.10 definition: is water found with a vessel and a board hopes with able is board 

*********
CITATIONS
*********

1)Our main idea for clustering uses HDP (Hierarchical Dirichlet process), an algorithm developed by Yee Whye Teh, Michael I. Jordan, Matthew J. Beal and David Blei:
 Hierarchical dirichlet processes
(http://www.gatsby.ucl.ac.uk/~ywteh/research/npbayes/jasa2006.pdf)

@article{teh2012hierarchical,
  title={Hierarchical dirichlet processes},
  author={Teh, Yee Whye and Jordan, Michael I and Beal, Matthew J and Blei, David M},
  journal={Journal of the american statistical association},
  year={2012},
  publisher={Taylor \& Francis}
}


2) Our implementaion uses a python package developed by Jey Han Lau, Paul Cook, Diana McCarthy, David Newman and Timothy Baldwin that that utilizes the HDP implementation of the previous authors to perform WSI: 
Word sense induction for novel sense detection
(http://www.ics.uci.edu/~newman/pubs/eacl2012.pdf)

@inproceedings{lau2012word,
  title={Word sense induction for novel sense detection},
  author={Lau, Jey Han and Cook, Paul and McCarthy, Diana and Newman, David and Baldwin, Timothy},
  booktitle={Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics},
  pages={591--601},
  year={2012},
  organization={Association for Computational Linguistics}
}




